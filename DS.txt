1.
>  library("sofa")
>  install.packages("sofa")

Type http://localhost:5984/_utils/#login on the browser to verify installation
>  x<-Cushion$new(user="bhavans",pwd="bhavans")
> x$ping()

Creating database ds
>  db_create(x,dbname='ds')
>   db_list(x)

Entring records in database
>  doc1<-'{"rollno":"01","name":"xyz","Grade":"A"}'
>  doc_create(x,doc1,dbname="ds",docid="a_1")
>    doc2<-'{"rollno":"02","name":"Vibhuti","Grade":"B"}'
>  doc_create(x,doc2,dbname="ds",docid="a_2")
>  doc4<-'{"rollno":"04","name":"Karan","Grade":"C","remark":"Pass"}'
>  doc_create(x,doc4,dbname="ds",docid="a_4")

Changing feed
db_changes(x,"ds")

GREATER THAN  
db_query(x,dbname="ds",selector=list('_id'=list('$gt'=NULL)))$docs

Searching student with mark pass
>  db_query(x,dbname="ds",selector =list(remark="Pass"))$docs
roll no greater than 2 and only name and grade
>  db_query(x,dbname="ds",selector =list(rollno=list('$gt'='02')),fields=c("name","Grade"))$docs

Displaying json doc in table format
>  library("jsonlite")
>  res<-db_query(x,dbname="ds",selector=list('_id'=list('$gt'=NULL)),fields=c("name","rollno","Grade","remark "),as="json")
>  fromJSON(res)$docs

DELETE DOC
>  doc_delete(x,dbname="ds",docid="a_3")
getting deleted doc
>  doc_get(x,dbname="ds",docid="a_3")

Changing the content of document
>  doc2<-'{"name":"food","Pizza":"TEST","note":"yummy","note2":"yay"}'
>  doc_update(x,dbname="ds",doc=doc2,docid="a_2",rev="1-ac63e6cf941846b3def9f1260c995f8e")

2.
Step1: Create a folder in C drive named data, create new folder db in it.

Step 2: Open the path of Mongodb C:\Program Files\MongoDB\Server\5.0\bin
use studentinfo
show dbs
db.student.insert({name:"Sahil",age:20})
db.dropDatabase()
db.student.insert({name:"Vibhuti" , age:20, marks:707, college:”Bhavans” })
show collections
db.employee.insert({name:"Noel",post:”Software Developer"})
db.employee.drop()
db.student.find()
db.student.find().forEach(printjson)
db.student.find().pretty()
db.student.find({"age":{$gt:20}}).pretty()
db.student.find({"age":{$lt:21}}).pretty()
db.student.find({"age":{$ne:20}}).pretty()
db.student.find({"age":{$gte:20}}).pretty()
db.student.find({"age":{$lte:21}}).pretty()
db.student.update({"name":"sahil"},{$set:{"name":"sahil mirza"}})
db.student.remove({"name":"hetvi"})
db.student.remove({})

3.Practical of Principal Component Analysis
> data_iris <-iris[1:4]                                               #Taking the numeric part of the IRIS data

> Cov_data <-cov(data_iris)                                    #Calculating the covariance matrix

> Eigen_data<-eigen(Cov_data)           #Find out the eigen vectors and eigen values using the covariance matrix

> Pca_data<-princomp(data_iris,cor=“False”)          #Using the inbuilt function PCA calculation
> Eigen_data$values

Squaring the data
> Pca_data $sdev^2

Comparing the eigen vectors
> Pca_data $loadings[,1:4]
> Eigen_data $vectors
> summary(Pca_data)

Plotting
> biplot(Pca_data) 
> screeplot(Pca_data,type="line")

Calculating the difference in accuracy between these 2 modules
>biplot(Pca_data)
>screenplot(Pca_data,type=”line”)
                                    #Selecting the first principal component for the second model
>model2=Pca_data$loading[,1]
  #For second model, we need to calculate scores by multiplying our loadings with the data
> model2_scores<-as.matrix(data_iris)%*%model12
#Loading libraries for naiveBayes model
>library(class)
>install.packages(“e1071”)

 Fitting the 1st model over the entire data 
> mod1<-navieBayes(iris[,1:4],iris[,5])

Fitting 2nd model using the first principal component 
> mod2<-navieBayes(iris[model2_scores,iris[,5])

 Printing model 1, model 2
> mod1
> mod2

If the size of data is reduced that is mod2 our accuracy also reduces (diagonal value)
Accuracy for the 1st model
> table(predict(mod1,iris[,1:4]),iris[,5]))
 Accuracy for the 2nd model
> table(predict(mod2,model2_scores),iris[,5])]

4.Practical of Clustering
>install.packages()
>library(ggplot2)
>scatter<-ggplot(data=iris,aes(x=Sepal.Length,y=Sepal.Width))
>scatter
>scatter+geom_point(aes(color=Species,shape=Species))+theme_bw()+xlab("Sepal Length")+ylab("Sepal Width")+ggtitle("Sepal Length-Width")
>ggplot(data=iris,aes(Sepal.Length,fill=Species))+theme_bw()+geom_density(alpha=0.25)+labs(x="Sepal.Length",title="Species vs Sepal Length")
>vol+stat_density(aes(ymax=..density..,ymin=-..density..,fill=Species,color=Species),geom="ribbon",position="identity")+facet_grid(.~Species)+coord_flip()+theme_bw()+labs(x="Sepal Length", title="Species vs Sepal Length")
>vol<-ggplot(data=iris,aes(x=Sepal.Width))
>vol+stat_density(aes(ymax=..density..,ymin=-..density..,fill=Species,color=Species),geom="ribbon",position="identity")+facet_grid(.~Species)+coord_flip()+theme_bw()+labs(x="Sepal Width", title="Species vs Sepal Width")

Applying Clustering Algorithm
>irisData<-iris[,1:4]
>totalwSS<-c()
#Method 1 kmeans clustering for 15 times in a loop
>for(i in 1:15)
>+ {
>+ clusterIRIS<-kmeans(irisData,centers=i)
>+ totalwSS[i]<-clusterIRIS$tot.withinss
>+ }

#Use plot function to plot values of tot_wss against no-of-clusters
>plot(x=1:15,y=totalwSS,type="b",xlab="Number of Clusters", ylab="Within groups sum-of-squares") 



•	Number of clusters = 15

•	Means 15 different types of data is present

•	The above graph shows that these is only 1 cluster of data which has higher sum of square values .i.e. the one near 700

•	The other clusters are mainly lying between 0 to 100

Method 2 - Using NbClust – Uses huge number of cluster suitability measuring criteria
install.packages("NbClust")
library(NbClust)

#Set margins as : c(bottom,left,top,right)
par(mar=c(2,2,2,2))
nb<-NbClust(irisData,method="kmeans")

Histogram
 hist(nb$Best.nc[1,],breaks=15,main="Histogram for number of clusters")

Method 3 - Clustering Data with Silhouette plot
#Here we are calculating distance 2 Clusters
>library(cluster)
>cl<-kmeans(iris[,-5],2)  # here 2 means 2 clusters

#Compute and returns the distance matrix computed by using Euclidean distance measure to 
#the distance between the rows of a data matrix
>dis<-dist(iris[,-5])^2
>sil=silhouette(cl$cluster,dis)
>plot(sil,main="Clustering Data with Silhoutte plot using 2 Clusters", col=c("cyan","blue"))

8 Clusters
>library(cluster)
>cl<-kmeans(iris[,-5],8)
>dis<-dist(iris[,-5])^2
>sil=silhouette(cl$cluster,dis)
>plot(sil,main="Clustering Data with Silhoutte plot using 8 Clusters", col=c("cyan","blue","orange","yellow","red","gray","green","maroon"))

5.Practical of Time Series Forcasting.
>data("AirPassengers")
>class(AirPassengers)
> start(AirPassengers)
> end(AirPassengers)
> frequency(AirPassengers)
> summary(AirPassengers)
> plot(AirPassengers)
> abline(reg=lm(AirPassengers~time(AirPassengers)))
> cycle(AirPassengers)
> plot(aggregate(AirPassengers,FUN=mean))
> boxplot(AirPassengers~cycle(AirPassengers))
> acf(log(AirPassengers))
> acf(diff(log(AirPassengers)))
> (fit <- arima(log(AirPassengers),c(0,1,1),seasonal=list(order=c(0,1,1),period=12)))
>pred<-predict(fit,n.ahead=10*12)
> ts.plot(AirPassengers,2.718^pred$pred,log="y",lty=c(1,3))

6.Practical of Simple/Multiple Linear Regression
> lsfit(iris$Petal.Length, iris$Petal.Width)$coefficients
>plot(iris$Petal.Length,iris$Petal.Width,pch=21,bg=c("red","green3","blue")[unclass(iris$Species)],main="Iris Data",xlab="Petal length",ylab="Petal Width")
> abline(lsfit(iris$Petal.Length,iris$Petal.Width)$coefficients,col="black")
> summary(lm(Petal.Width~Petal.Length,data=iris))
• Summary function gives residuals which is the measure of the distance between the 
line starting at x axis and the origin

Sepal
>plot(iris$Sepal.Width,iris$Sepal.Length,pch=21,bg=c("red","green3","blue")[unclass(iris$Species)],main="Iris Data",xlab="Sepal Width",ylab="Sepal Length")
> abline(lm(Sepal.Length~Sepal.Width,data=iris)$coefficients,col="black")
> summary(lm(Sepal.Length~Sepal.Width,data=iris))
>plot(iris$Sepal.Width,iris$Sepal.Length,pch=21,bg=c("red","green3","blue")[unclass(iris$Species)],main="Iris Data",xlab="Sepal Width",ylab="Sepal Length")
> abline(lm(iris$Sepal.Length~iris$Sepal.Width,data=iris)$coefficients,col="black")
> summary(lm(Sepal.Length~Sepal.Width,data=iris))

#Plotting on group of  data such as setosa,virginica,versicolor therefore three lines will be plotted.
>plot(iris$Sepal.Width,iris$Sepal.Length,pch=21,bg=c("red","green3","blue")[unclass(iris$Species)],main="Iris Data",xlab="Sepal Width",ylab="Sepal Length")
> abline(lm(iris$Sepal.Length~iris$Sepal.Width,data=iris)$coefficients,col="black")
>abline(lm(Sepal.Length~Sepal.Width,data=iris[which(iris$Species=="setosa"),])$coefficients,col="red")
>abline(lm(Sepal.Length~Sepal.Width,data=iris[which(iris$Species=="versicolor"),])$coefficients,col="green3")
>abline(lm(Sepal.Length~Sepal.Width,data=iris[which(iris$Species=="virginica"),])$coefficients,col="blue")
lm(Sepal.Length~Sepal.Width,data=iris[which(iris$Species=="setosa"),])$coefficients lm(Sepal.Length~Sepal.Width,data=iris[which(iris$Species=="versicolor"),])$coefficients lm(Sepal.Length~Sepal.Width,data=iris[which(iris$Species=="virginica"),])$coefficients
> lm(Sepal.Length~Sepal.Width:Species+Species-1,data=iris)$coefficients 
> summary(lm(Sepal.Length~Sepal.Width:Species+Species-1,data=iris))
# Simplify with AIC(Akaike Information Criterion)
> summary(step(lm(Sepal.Length~Sepal.Width*Species,data=iris)))
lm(Sepal.Length~Sepal.Width:Species+Species-1,data=iris)$coefficients lm(Sepal.Length~Sepal.Width:Species+Species,data=iris)$coefficients

7.Practical of Logistics Regression
library(datasets)
ir_data<-iris
head(ir_data)
str(ir_data)
levels(ir_data$Species)
sum(is.na(ir_data))
 ir_data<-ir_data[1:100,]
 set.seed(100)
samp<-sample(1:100,80)
ir_test<-ir_data[samp,]
ir_ctrl<-ir_data[-samp,]
>  install.packages()
>  library(ggplot2)
>  library(GGally)
>  ggpairs(ir_test)
>#Checking model’s prediction
>  y<-ir_test$Species;x<-ir_test$Sepal.Length
>  glfit<-glm(y~x,family='binomial')
>  summary(glfit)
>#predict using this mode
>  newdata<-data.frame(x=ir_ctrl$Sepal.Length)
>  predicted_val<-predict(glfit,newdata,type="response")
>  prediction<-data.frame(ir_ctrl$Sepal.Length,ir_ctrl$Species,predicted_val)
prediction
>  qplot(prediction[,1],round(prediction[,3]), col=prediction[,2], xlab='Sepal Length', ylab='Prediction using logistic Reg.')

8.
Practical of Hypothesis testing
Create 1 hypothesis set of data - Perform t test on the set
> x=c(6.2,6,6,7.1,7.4,7.6,7.9,8,8.3,8.4,8.5,8.6,8.8,8.8,9.1,9.2,9.4,9.4,9.7,9.9,10.2,10.4,10.8,11.3,11.9)
> t.test(x-9,alternative="two.sided",conf.level=0.95)
Create 2 hypothesis set of data Perform t test on the set
> x=c(418,421,421,422,425,427,431,434,437,439,446,447,448,453,454,463,465)
> y=c(429,430,430,431,36,437,440,441,445,446,447)
> t.test(x,y,alternative="two.sided",mu=0,var.equal=F,conf.level=0.95)

9.
Practical of Analysis of Variance
Declaring 3 vector variable
> y1=c(18.2,20.1,17.6,16.8,18.8,19.7,19.1)
> y2=c(17.4,18.7,19.1,16.4,15.9,18.4,17.7)
> y3=c(15.2,18.8,17.7,16.5,15.9,17.1,16.7)
each vector is having 7 Values
c here represents that it is a vector
> y=c(y1,y2,y3)
> n=rep(7,3)
> n
rep will show the number of values in the vector

1 would be displayed 7 times and so do 2 and 3 because there are 7 values in each vector
> group=rep(1:3,n)
> group

Stem-Leaf Diagram
> tmp=tapply(y,group,stem)
> stem(y)

Calculating the Summary i.e sum, mean, variance
> tmpfn=function(x)c(sum=sum(x),mean=mean(x),var=var(x),n=length(x))
> tapply(y,group,tmpfn)

Analysis of Variance Table, lm data is linearly seperated
> data=data.frame(y=y,group=factor(group))
> fit=lm(y~group,data)
> anova(fit)
Calculating the Error
> df=anova(fit)[,"Df"]
> names(df)=c("trt","err")
> df
0.05 is the confidence value
> alpha=c(0.05,0.01)
> qf(alpha,df["trt"],df["err"],lower.tail=FALSE)
> anova(fit)["Residuals","Sum Sq"]
> anova(fit)["Residuals","Sum Sq"]/qchisq(c(0.025,0.975),18,lower.tail=FALSE)

10.a
install.packages("party")
library(party)
print(head(readingSkills))
input.dat<-readingSkills[c(1:105),]
png(file="decision_tree.png")
output.tree<-ctree(nativeSpeaker~age+shoeSize+score,data=input.dat)
print(output.tree)
dev.off()
plot(output.tree)

b.
install.packages("rpart")
library(rpart)
myData<-data.frame(iris)
attach(myData)
library(rpart)
model<-
rpart(Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,data=myData
,method="class")
plot(model)
text(model,use.n=TRUE,all=TRUE,cex=0.8)

c.
install.packages("tree")
library(tree)
model1<-
tree(Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,data=myData,
method="class",split="gini")
plot(model1)
text(model1,all=TRUE,cex=0.6)
tree(Species~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width,data=myData,
method="class",control=tree.controls(nobs=150,mincut=10))
plot(model)
text(model,all=TRUE,cex=0.6)

