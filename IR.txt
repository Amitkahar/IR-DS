1.
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
docs=['How ARE you yash','i am GOOD','HAVE A GOOD DAY']
print(docs)
vec=CountVectorizer()
x=vec.fit_transform(docs)
print("Vocubulary: ",vec.vocubulary)
print(x)

df=pd.DataFrame(x.toarray(),columns=vec.get_feature_names_out())
print(df)

w1=input("Enter word1: ")
w2=input("Enter word1: ")
op=input("Enter Operator: ")

x=[]

for i in range(df.shape[0]):
    if(op=="&"):
        a=(list(df.loc[:,w1]))[i]&(list(df.loc[:,w2]))[i]
        x.append(a)
    elif(op=="|"):
        a=(list(df.loc[:,w1]))[i](list(df.loc[:,w2]))[i]
        x.append(a)
        
print(x)

for i in range(df.shape[0]):
    if(x[i]==1):
        print("Doc",i)

1.b
a = 60
b = 13
c = 0

c = a&b
print("Value of a & b is: ", c)

c = a|b
print("Value of a | b is: ", c)

c = a^b
print("Value of a ^ b is: ", c)

c =~a
print("Value of ~a is: ", c)

c = a<<2
print("Value of a<<2 is: ", c)

c = a>>2
print("Value of a>>2 is: ", c)

2.
import numpy as np
from fractions import Fraction
def display_format(my_vector, my_decimal):
    return np.round((my_vector).astype(float), decimals=my_decimal)
dp = Fraction(1,3)
M = np.matrix([[0, Fraction(1,2), Fraction(1, 2)],
[1, 0, 0],
[1, 0, 0]]
)
print(M)
E = np.zeros((3,3))
E[:] = dp
beta = 0.8

A = beta * M + ((1-beta) * E)
r = np.matrix([dp, dp, dp])
r = np.transpose(r)
previous_r = r
for i in range(1,5):
    r = A * r
    print(r)
    print (display_format(r,3))
    if (previous_r==r).all():
        break
previous_r = r
print ("Final:\n", display_format(r,3))
print ("sum", np.sum(r))

3.
def editdistance(str1,str2,m,n):
    if m==0:
        return n
    if n==0:
        return m
    if str1[m-1] == str2[n-1]:
        return editdistance(str1,str2,m-1,n-1)
    return 1 + min(editdistance(str1,str2,m,n-1),
    editdistance(str1,str2,m-1,n),editdistance(str1,str2,m-1,n-1))
str1=input("Enter string 1: ")
str2=input("Enter string 2: ")
print("The edit Distance is : ", 
editdistance(str1,str2,len(str1),len(str2)), "between",str1,"and",str2)    

4.
import math
import string
import sys
# reading the text file  # This functio will return a list of the lines of text in the file.

def read_file(filename):
	try:
		with open(filename, 'r') as f:
			data = f.read()
		return data
	except IOError:
		print("Error opening or reading input file: ", filename)
		sys.exit()

# splitting the text lines into words, translation table is a global variable
# mapping upper case to lower case and, punctuation to spaces

translation_table = str.maketrans(string.punctuation+string.ascii_uppercase,
					""*len(string.punctuation)+string.ascii_lowercase)
	
# returns a list of the words in the file

def get_words_from_line_list(text):	
	text = text.translate(translation_table)
	word_list = text.split()
	return word_list

# counts frequency of each word returns a dictionary which mapsthe words to their frequency.

def count_frequency(word_list):
	D = {}	
	for new_word in word_list:		
		if new_word in D:
			D[new_word] = D[new_word] + 1			
		else:
			D[new_word] = 1			
	return D

# returns dictionary of (word, frequency) pairs from the previous dictionary.
def word_frequencies_for_file(filename):
	
	line_list = read_file(filename)
	word_list = get_words_from_line_list(line_list)
	freq_mapping = count_frequency(word_list)
	print("File", filename, ":", )
	print(len(line_list), "lines, ", )
	print(len(word_list), "words, ", )
	print(len(freq_mapping), "distinct words")
	return freq_mapping

# returns the dot product of two documents
def dotProduct(D1, D2):
	Sum = 0.0
	for key in D1:	
		if key in D2:
			Sum += (D1[key] * D2[key])		
	return Sum

# returns the angle in radians between document vectors

def vector_angle(D1, D2):
	numerator = dotProduct(D1, D2)
	denominator = math.sqrt(dotProduct(D1, D1)*dotProduct(D2, D2))
	return math.acos(numerator / denominator)
def documentSimilarity(filename_1, filename_2):
	
# filename_1 = sys.argv[1]
# filename_2 = sys.argv[2]

	sorted_word_list_1 = word_frequencies_for_file(filename_1)
	sorted_word_list_2 = word_frequencies_for_file(filename_2)
	distance = vector_angle(sorted_word_list_1, sorted_word_list_2)
	bg=distance*180/3.14
	print("The distance between the documents is: % 0.6f (radians)"% distance,"\nIn Degrees: ",bg)
    
# Driver code
documentSimilarity('s1.txt', 's2.txt')

5.
from collections import Counter 
import re
#initializing string 
test=input("Enter a string: ") 
string=re.sub("[^a-zA-Z]+", "",test) 
print("\nString: ",string)
#[^a-zA-Z]+" look fro any group of characters that are not alphabets 
#"" - replace any matched character with ""
#using collections.Counter() to get count of each elements in the string 
res=Counter(string.casefold())
print("\nOutput: ",str(res))

6.
import nltk 
nltk.download('stopwords') 
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
file = open('sample.txt')
send = file.read()
stop = set(stopwords.words('english')) 
token = word_tokenize(send)
a = []
for w in token:
    if w not in stop: 
        a.append(w)
print("\nOriginal sentence: ",token) 
print("=================================================================================")
print("\nStop words are: \n",stop) 
print("=================================================================================")
print("\nAfter the removal of Stop words we have: \n",a,"\n")
print("=================================================================================")
print(" ".join(a))

8.
import requests
from bs4 import BeautifulSoup
plain_text=requests.get('https://www.soundstripe.com').text 
s=BeautifulSoup(plain_text,"html.parser")
for link in s.findAll('img'): 
    tet=link.getText()
print(tet) 
print(link) 
print(plain_text)

9.
import csv
import requests
import xml.etree.ElementTree as ET

def loadRSS():

	url = 'https://timesofindia.indiatimes.com/rssfeedsvideo/3813456.cms'
	resp = requests.get(url)
	with open('topnewsfeed.xml', 'wb') as f:
		f.write(resp.content)
		
def parseXML(xmlfile):
	tree = ET.parse(xmlfile)
	root = tree.getroot()
	newsitems = []
	for item in root.findall('./channel/item'):
		news = {}
		for child in item:
			if child.tag == '{http://search.yahoo.com/mrss/}content':
				news['media'] = child.attrib['url']
			else:
				news[child.tag] = child.text.encode('utf8')
		newsitems.append(news)
	return newsitems

def savetoCSV(newsitems, filename):
	fields = ['guid', 'title', 'pubDate', 'description', 'link', 'media']
	with open(filename, 'w') as csvfile:
		writer = csv.DictWriter(csvfile, fieldnames = fields)
		writer.writeheader()
		writer.writerows(newsitems)
	
def main():
	loadRSS()
	newsitems = parseXML('topnewsfeed.xml')
	savetoCSV(newsitems, 'topnews.csv')	
	
if __name__ == "__main__":
	main()

